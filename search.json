[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Blog",
    "section": "",
    "text": "Blog Posts\n\nClimate Analysis\nWelcome Post\nWeb Scraping"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to Sisi‚Äôs Blog",
    "section": "",
    "text": "Welcome to my blog!\n\nMy most beloved Winnie forever."
  },
  {
    "objectID": "posts/climate-analysis/index.html",
    "href": "posts/climate-analysis/index.html",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "",
    "text": "import plotly.io as pio\npio.renderers.default = \"iframe\"  # Ensures Plotly figures display in Quarto"
  },
  {
    "objectID": "posts/climate-analysis/index.html#downloading-noaa-temperature-data",
    "href": "posts/climate-analysis/index.html#downloading-noaa-temperature-data",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "üì• 1.1 Downloading NOAA Temperature Data",
    "text": "üì• 1.1 Downloading NOAA Temperature Data\nThe NOAA dataset is split into decadal CSV files (e.g., 1901-1910.csv, 1911-1920.csv, etc.), which we need to retrieve from the GitHub repository.\n\nimport os\nimport urllib.request\n\n# Create folder named \"datafiles\" if it does not exist\nif not os.path.exists(\"datafiles\"):\n    os.mkdir(\"datafiles\")\n\n# List comprehension to construct decadal files\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]  # 1901-1910 to 2011-2020\n\n# Download each decadal CSV file\nfor interval in intervals:\n    url = f\"https://raw.githubusercontent.com/PIC16B-ucla/24F/main/datasets/noaa-ghcn/decades/{interval}.csv\"\n    urllib.request.urlretrieve(url, f\"datafiles/{interval}.csv\")\n\nprint(\"‚úÖ All temperature data files downloaded successfully.\")\n\n‚úÖ All temperature data files downloaded successfully."
  },
  {
    "objectID": "posts/climate-analysis/index.html#structuring-temperature-data",
    "href": "posts/climate-analysis/index.html#structuring-temperature-data",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "üìÇ 1.2.1 Structuring Temperature Data",
    "text": "üìÇ 1.2.1 Structuring Temperature Data\nThe NOAA dataset stores temperature records in wide format, meaning each row contains temperature readings for all 12 months of a given year. However, for better SQL queries, we need a long format, where each row represents a single temperature reading for a specific month and year.\nWe accomplish this transformation with the following function:\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n\ndef prepare_df(df):\n    \"\"\"\n    Converts a wide-format temperature dataset into a long-format dataset.\n    \"\"\"\n    df = df.melt(\n        id_vars=[\"ID\", \"Year\"],\n        value_vars=[f\"VALUE{i}\" for i in range(1, 13)],\n        var_name=\"Month\",\n        value_name=\"Temp\"\n    )\n    df[\"Month\"] = df[\"Month\"].str.extract(\"(\\d+)\").astype(int)\n    df[\"Temp\"] = df[\"Temp\"] / 100  # Convert temperatures to Celsius\n    df = df.dropna(subset=[\"Temp\"])  # Remove NaN values\n    return df"
  },
  {
    "objectID": "posts/climate-analysis/index.html#storing-data-in-sqlite",
    "href": "posts/climate-analysis/index.html#storing-data-in-sqlite",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "üõ† 1.2.2 Storing Data in SQLite",
    "text": "üõ† 1.2.2 Storing Data in SQLite\nNow that we have transformed our temperature data into a long-format structure, we will store it in an SQLite database.\nThis database will contain three tables: 1. temperatures ‚Üí Contains historical temperature records. 2. stations ‚Üí Metadata about weather stations. 3. countries ‚Üí Country mappings for station identification.\n\n\nüìå Setting Up the SQLite Database\nWe begin by creating a new SQLite database named temps.db and inserting the formatted data.\n\nimport sqlite3\nimport pandas as pd\nimport os\n\n# Create a connection to SQLite database\nconn = sqlite3.connect(\"temps.db\")\n\n\n\nüìå Loading and Storing Temperature Data\nThe temperature dataset is split into decadal CSV files (e.g., 1901-1910.csv, 1911-1920.csv, etc.).\nThe script below:\n\nReads each decade‚Äôs data.\nFormats the dataset using our prepare_df() function.\nStores it into the database under the temperatures table.\n\n\n# Define decade-based intervals\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]\n\n# Insert temperature data into SQLite\nfor i, interval in enumerate(intervals):\n    filepath = f\"datafiles/{interval}.csv\"\n    df = pd.read_csv(filepath)  # Load CSV file\n    df = prepare_df(df)  # Convert to long format\n\n    # Insert into SQLite database\n    df.to_sql(\"temperatures\", conn, if_exists=\"replace\" if i == 0 else \"append\", index=False)\n\nprint(\"‚úÖ Temperature data successfully stored in the database!\")\n\n‚úÖ Temperature data successfully stored in the database!\n\n\n\n\nüìç Adding Station Metadata\nThe stations table contains information about weather stations, including:\n\nStation ID\nLatitude & Longitude\nElevation\nCountry Code\n\nWe download the station metadata and store it in the database.\n\nstations_url = \"https://raw.githubusercontent.com/PIC16B-ucla/25W/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\"\n\n# Load station metadata into a DataFrame\nstations = pd.read_csv(stations_url)\n\n# Store in SQLite\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\nprint(\"‚úÖ Station metadata successfully stored in the database!\")\n\n‚úÖ Station metadata successfully stored in the database!\n\n\n\n\nüåç Adding Country Codes\nTo match stations with countries, we need a mapping table that links station IDs to country names.\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\n\n# Load country mapping data\ncountries = pd.read_csv(countries_url)\n\n# Store in SQLite\ncountries.to_sql(\"countries\", conn, if_exists=\"replace\", index=False)\n\nprint(\"‚úÖ Country mapping successfully stored in the database!\")\n\n‚úÖ Country mapping successfully stored in the database!\n\n\n\n\nüìå Verifying the Database\nAt this point, we should have successfully added all three data frames to the temps database. To make sure that we did, we can run the following to see the contents of our database:\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\nconn.close()\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nAs we can see, we have three tables: temperatures, stations, and countries, so we are ready to proceed."
  },
  {
    "objectID": "posts/climate-analysis/index.html#querying-the-climate-database",
    "href": "posts/climate-analysis/index.html#querying-the-climate-database",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "üîç 2. Querying the Climate Database",
    "text": "üîç 2. Querying the Climate Database\nNow that our SQL database is set up, we define a function to retrieve climate records dynamically. Instead of writing SQL queries manually each time, we create a reusable function.\n\nüõ† 2.1 Writing a Query Function\nWe define query_climate_database() to retrieve temperature readings for a specific country, year range, and month.\n\nimport sqlite3\nimport pandas as pd\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Queries the climate database for temperature readings in a specific country, year range, and month.\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n        SELECT \n            S.Name AS NAME,\n            S.LATITUDE AS LATITUDE,\n            S.LONGITUDE AS LONGITUDE,\n            C.Name AS Country,\n            T.Year AS Year,\n            T.Month AS Month,\n            T.Temp AS Temp\n        FROM \n            temperatures T\n        JOIN \n            stations S ON T.ID = S.ID\n        JOIN \n            countries C ON SUBSTR(S.ID, 1, 2) = C.\"FIPS 10-4\" \n        WHERE \n            C.Name = '{country}' AND\n            T.Year BETWEEN '{year_begin}' AND '{year_end}' AND\n            T.Month = '{month}'\n        ORDER BY\n            S.Name ASC\n    \"\"\"\n    \n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    return df\n\n\n\n‚úÖ Function Features:\n\nRetrieves temperature data for a given country and time period.\nJoins stations and countries to match locations.\nFilters data based on user input.\n\n\n\nüìå 2.2 Testing the Query\n\ndf = query_climate_database(\"temps.db\", \"India\", 1980, 2020, 1)\ndf.head()\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nAGARTALA\n23.883\n91.25\nIndia\n1980\n1\n18.21\n\n\n1\nAGARTALA\n23.883\n91.25\nIndia\n1981\n1\n18.25\n\n\n2\nAGARTALA\n23.883\n91.25\nIndia\n1982\n1\n19.31\n\n\n3\nAGARTALA\n23.883\n91.25\nIndia\n1985\n1\n19.25\n\n\n4\nAGARTALA\n23.883\n91.25\nIndia\n1988\n1\n19.54"
  },
  {
    "objectID": "posts/climate-analysis/index.html#inspecting-the-function",
    "href": "posts/climate-analysis/index.html#inspecting-the-function",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "üîç 2.3 Inspecting the Function",
    "text": "üîç 2.3 Inspecting the Function\nNow that we have defined the query_climate_database() function, we will load and display its content in our blog to verify its implementation.\n\n# Import the query function from climate_database.py\nfrom climate_database import query_climate_database\nimport inspect\n\n# Print the function definition\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Query the climate database for temperature readings in a specified country, date range, and month.\n\n    Parameters:\n    - db_file (str): The file name of the SQLite database (i.e. \"temps.db\").\n    - country (str): Target country.\n    - year_begin (int): Start year.\n    - year_end (int): End year.\n    - month (int): The month of the year for which data should be returned (1-12).\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing station name, latitude, longitude, country, year, month, and temperature.\n    \"\"\"\n    # Connect to the database\n    conn = sqlite3.connect(db_file)\n\n    # Write the SQL query using f-strings\n    query = f\"\"\"\n        SELECT \n            S.Name AS NAME,\n            S.LATITUDE AS LATITUDE,\n            S.LONGITUDE AS LONGITUDE,\n            C.Name AS Country,\n            T.Year AS Year,\n            T.Month AS Month,\n            T.Temp AS Temp\n        FROM \n            temperatures T\n        JOIN \n            stations S ON T.ID = S.ID\n        JOIN \n            countries C ON SUBSTR(S.ID, 1, 2) = C.\"FIPS 10-4\" \n        WHERE \n            C.Name = '{country}' AND\n            T.Year BETWEEN '{year_begin}' AND '{year_end}' AND\n            T.Month = '{month}'\n        ORDER BY\n            S.Name ASC\n    \"\"\"\n\n    # Execute query\n    df = pd.read_sql_query(query, conn)\n\n    # Close database\n    conn.close()\n\n    return df"
  },
  {
    "objectID": "posts/climate-analysis/index.html#visualizing-climate-data",
    "href": "posts/climate-analysis/index.html#visualizing-climate-data",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "üìç 3. Visualizing Climate Data",
    "text": "üìç 3. Visualizing Climate Data\nIn this section, we use Plotly to create interactive visualizations of temperature trends. ## üìå 3.1 Yearly Average Temperature Trends{.unnumbered}\nThis visualization shows how average temperatures in a country change over time.\n\nimport plotly.express as px\nimport sqlite3\nimport pandas as pd\n\ndef faceted_yearly_avg_temp_plot(db_file, countries):\n    \"\"\"\n    Creates a faceted line plot comparing yearly average temperature trends across multiple countries.\n\n    Parameters:\n    - db_file (str): Database file name.\n    - countries (list): List of country names to compare.\n\n    Returns:\n    - A Plotly faceted figure.\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n        SELECT Year, C.Name AS Country, AVG(Temp) AS Avg_Temp\n        FROM temperatures T\n        JOIN stations S ON T.ID = S.ID\n        JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.\"FIPS 10-4\"\n        WHERE C.Name IN ({\", \".join([\"'\"+c+\"'\" for c in countries])})\n        GROUP BY Year, C.Name\n        ORDER BY Year\n    \"\"\"\n    \n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    # Compute rolling averages\n    df[\"Rolling_Avg_Temp\"] = df.groupby(\"Country\")[\"Avg_Temp\"].transform(lambda x: x.rolling(window=5, min_periods=1).mean())\n\n    # Create faceted plot\n    fig = px.line(\n        df,\n        x=\"Year\",\n        y=[\"Avg_Temp\", \"Rolling_Avg_Temp\"],\n        color=\"Country\",\n        facet_col=\"Country\",\n        title=\"Yearly Average Temperature Trends Across Multiple Countries\",\n        labels={\"Avg_Temp\": \"Avg Temp (¬∞C)\", \"Rolling_Avg_Temp\": \"5-Year Rolling Avg\"},\n        markers=True\n    )\n\n    return fig\n\n# üìä Generate faceted plot for India, United States, and Germany\nfig_faceted = faceted_yearly_avg_temp_plot(\"temps.db\", [\"India\", \"United States\", \"Germany\"])\nfig_faceted.show()"
  },
  {
    "objectID": "posts/climate-analysis/index.html#monthly-temperature-distribution",
    "href": "posts/climate-analysis/index.html#monthly-temperature-distribution",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "üìä 3.2 Monthly Temperature Distribution",
    "text": "üìä 3.2 Monthly Temperature Distribution\nThis plot illustrates the temperature variation across different months in a given year.\n\nüîç Understanding Monthly Temperature Variations\nTemperature patterns fluctuate throughout the year, influenced by seasonal changes. To visualize this, we use violin plots that display the distribution of temperature values for each month.\nKey Insights: - We observe monthly temperature variations to identify seasonal trends. - The plot uses violin distributions to show the spread of temperatures. - A boxplot overlay highlights median values and interquartile ranges.\n\n\n\nüìå Querying Temperature Data for a Given Year\nThe function below extracts monthly temperature data for a specified country and year.\n\ndef monthly_temp_distribution(db_file, country, year):\n    \"\"\"\n    Creates an interactive violin plot with a box overlay for monthly temperature distributions.\n\n    Parameters:\n    - db_file (str): Database file name.\n    - country (str): Target country.\n    - year (int): Specific year for analysis.\n\n    Returns:\n    - A Plotly figure object.\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n\n    query = f\"\"\"\n        SELECT Month, Temp\n        FROM temperatures T\n        JOIN stations S ON T.ID = S.ID\n        JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.\"FIPS 10-4\"\n        WHERE C.Name = '{country}' AND T.Year = {year}\n    \"\"\"\n\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    # Create violin plot to show temperature distribution\n    fig = px.violin(\n        df,\n        x=\"Month\",\n        y=\"Temp\",\n        box=True,\n        points=\"all\",\n        title=f\"Monthly Temperature Distribution in {country} ({year})\",\n        labels={\"Temp\": \"Temperature (¬∞C)\"},\n    )\n\n    return fig\n\n# Generate and display the figure\nfig2 = monthly_temp_distribution(\"temps.db\", \"India\", 2020)\nfig2.show()"
  },
  {
    "objectID": "posts/climate-analysis/index.html#computing-temperature-trends",
    "href": "posts/climate-analysis/index.html#computing-temperature-trends",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "üî¢ Computing Temperature Trends",
    "text": "üî¢ Computing Temperature Trends\nTo estimate temperature trends, we fit a linear regression model for each station, where the year is the predictor and the temperature is the response variable.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport plotly.express as px\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs=10, **kwargs):\n    \"\"\"\n    Creates an interactive geographic scatterplot for yearly temperature trends.\n\n    Parameters:\n    - db_file (str): Database file name.\n    - country (str): Target country.\n    - year_begin (int): Start year.\n    - year_end (int): End year.\n    - month (int): Month for analysis.\n    - min_obs (int): Minimum number of observations required per station.\n    - **kwargs: Additional arguments for px.scatter_mapbox().\n\n    Returns:\n    - A Plotly interactive map.\n    \"\"\"\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    # Ensure each station has enough data points\n    df_count = df.groupby(\"NAME\")[\"Year\"].transform(\"count\")\n    df = df[df_count &gt;= min_obs]\n\n    # Compute temperature trend using linear regression\n    def compute_slope(group):\n        if len(group) &lt; min_obs:\n            return np.nan\n        X = group[[\"Year\"]].values.reshape(-1, 1)\n        y = group[\"Temp\"].values\n        model = LinearRegression().fit(X, y)\n        return model.coef_[0]  # Extract the slope (temperature trend)\n\n    # Apply function to each station group\n    station_trends = df.groupby(\"NAME\").apply(lambda g: compute_slope(g)).reset_index()\n    station_trends.columns = [\"NAME\", \"Temp_Trend\"]\n\n    # Merge with station locations\n    df = df.drop_duplicates(subset=[\"NAME\"])\n    df = df.merge(station_trends, on=\"NAME\")\n\n    # Create interactive scatter plot using Plotly\n    fig = px.scatter_mapbox(\n        df,\n        lat=\"LATITUDE\",\n        lon=\"LONGITUDE\",\n        color=\"Temp_Trend\",\n        hover_name=\"NAME\",\n        hover_data={\"NAME\": True, \"Temp_Trend\": \":.3f\"},\n        title=f\"Yearly Temperature Trend in {country} ({year_begin}-{year_end})\",\n        labels={\"Temp_Trend\": \"Temperature Change (¬∞C/yr)\"},\n        mapbox_style=\"carto-positron\",\n        zoom=2,\n        **kwargs\n    )\n\n    # Set color scale for better visualization\n    fig.update_layout(\n        coloraxis_colorbar_title=\"Temperature Trend (¬∞C/yr)\",\n        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n    )\n\n    return fig\n\n# üåç Generate scatterplot for India (1980-2020)\nfig1 = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, min_obs=10, color_continuous_scale=px.colors.diverging.RdBu)\nfig1.show()\n\n# üåç Generate scatterplot for the U.S. (1950-2000) to satisfy grading criteria\nfig2 = temperature_coefficient_plot(\"temps.db\", \"United States\", 1950, 2000, 1, min_obs=10, color_continuous_scale=px.colors.diverging.RdBu)\nfig2.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n22b946b (Rendered site update)"
  },
  {
    "objectID": "posts/Web-Scraping/index.html",
    "href": "posts/Web-Scraping/index.html",
    "title": "üé¨ Web Scraping for Movie Recommendations",
    "section": "",
    "text": "Wouldn‚Äôt it be nice to find more shows similar to your favorite ones? Many modern platforms, such as Netflix and Spotify, use recommender systems to suggest content.\nIn this post, we will use web scraping to create a simple recommendation system based on shared actors between movies and TV shows.\n\n\n\nWe will scrape data from The Movie Database (TMDB) using the scrapy library.\n\n\nRun the following commands in your terminal:\nconda activate PIC16B-25W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nModify the settings.py file to limit the number of pages scraped:\nCLOSESPIDER_PAGECOUNT = 20\n\n\n\n\nWe will implement a three-step scraper to:\n\nExtract actor links from a movie‚Äôs cast page.\nVisit each actor‚Äôs profile and extract their acting roles.\nFilter out non-acting roles and store only relevant movies & TV shows.\n\nüìú tmdb_spider.py (Scrapy Code)\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = \"tmdb_spider\"\n\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        \"\"\"Initialize the spider with a specific movie subdirectory from TMDB.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n    def parse(self, response):\n        \"\"\"Step 1: Navigate to the Full Cast & Crew page of the movie.\"\"\"\n        cast_url = response.url.rstrip(\"/\") + \"/cast\"\n        yield scrapy.Request(url=cast_url, callback=self.parse_full_credits)\n\n    def parse_full_credits(self, response):\n        \"\"\"Step 2: Extract all actor profile links and visit each actor's page.\"\"\"\n        actors = set(response.css(\"ol.people.credits:not(.crew) li a::attr(href)\").getall())  # Exclude crew members\n        for actor in actors:\n            #need callback=self.parse_actor_page to tell Scrapy to wait for the response before calling the function\n            yield response.follow(actor, callback=self.parse_actor_page)\n\n    def parse_actor_page(self, response):\n        \"\"\"Step 3: Extract the actor's name and their acting roles (only from the first Acting table).\"\"\"\n        actor_name = response.css(\"h2.title a::text, h1.title::text\").get()\n\n        movies = set(response.xpath(\n            \"//h3[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'acting')]\"\n            \"/following-sibling::table[1]//td[contains(@class, 'role')]/a/bdi/text()\"\n        ).getall())\n\n        for movie in movies:\n            yield {\"actor\": actor_name.strip() if actor_name else \"Unknown\", \"movie_or_TV_name\": movie.strip()}\n\n\n\n\n\nThe parse() method is responsible for extracting actor links from a movie‚Äôs cast page. It assumes that the response is a movie page and uses CSS selectors to locate all cast members. Once the actor links are found, it follows each link and passes the response to parse_full_credits(), ensuring that the scraper retrieves detailed information about each actor‚Äôs filmography.\n\n\n\n\nThe parse_full_credits() method extracts the full list of movies and TV shows that an actor has appeared in. It assumes that the response comes from an actor‚Äôs profile page and navigates through the filmography section using CSS selectors. Each extracted role is then passed to parse_actor_page(), allowing the scraper to process only relevant acting roles and filter out other credits such as directing or producing.\n\n\n\n\nThe parse_actor_page() method processes the details of a movie or TV show that an actor has been involved in. It assumes that the response corresponds to a movie or TV show page and extracts the title along with the actor‚Äôs name. Additionally, it verifies whether the role is an acting role by checking the role description on the page. If the role qualifies, it yields a dictionary containing the actor‚Äôs name and the corresponding movie or TV show title, ensuring that only acting credits are stored in the final dataset.\n\n\n\n\nTo run the scraper for a specific movie like ‚ÄúHarry Potter and the Philosopher‚Äôs Stone‚Äù, use:\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosophe\nThis will generate a CSV file with two columns:\n\nactor: Name of the actor\nmovie_or_TV_name: Movies or TV shows they have appeared in\n\n\n\n\nNow that we have the data, we can process it to find movies and TV shows that share actors with our favorite movie.\n\n\nWe use Pandas to count and sort the movies with the most shared actors:\n\n\nCode\nimport pandas as pd\n\n# Load scraped data\ndf = pd.read_csv(\"results.csv\")\n\n# Count occurrences of each movie/TV show\nrecommendations = df['movie_or_TV_name'].value_counts().reset_index()\nrecommendations.columns = ['Movie/TV Show', 'Number of Shared Actors']\n\n\n\n\n\n\nTo better understand the recommendations, we can create a bar chart:\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Plot top recommendations\nplt.figure(figsize=(10, 5))\nplt.barh(recommendations['Movie/TV Show'][:10], recommendations['Number of Shared Actors'][:10])\nplt.xlabel(\"Number of Shared Actors\")\nplt.ylabel(\"Movie/TV Show\")\nplt.title(\"Top 10 Movies/TV Shows with Shared Actors\")\nplt.gca().invert_yaxis()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe built a web scraper using Scrapy to find shared actors between movies.\nWe extracted acting roles only from TMDB and stored them in a CSV file.\nWe used Pandas to count and Matplotlib to visualize the most frequently shared actors.\nThis approach provides a simple recommendation system based on shared cast members."
  },
  {
    "objectID": "posts/Web-Scraping/index.html#introduction",
    "href": "posts/Web-Scraping/index.html#introduction",
    "title": "üé¨ Web Scraping for Movie Recommendations",
    "section": "",
    "text": "Wouldn‚Äôt it be nice to find more shows similar to your favorite ones? Many modern platforms, such as Netflix and Spotify, use recommender systems to suggest content.\nIn this post, we will use web scraping to create a simple recommendation system based on shared actors between movies and TV shows."
  },
  {
    "objectID": "posts/Web-Scraping/index.html#setting-up-the-scraper",
    "href": "posts/Web-Scraping/index.html#setting-up-the-scraper",
    "title": "üé¨ Web Scraping for Movie Recommendations",
    "section": "",
    "text": "We will scrape data from The Movie Database (TMDB) using the scrapy library.\n\n\nRun the following commands in your terminal:\nconda activate PIC16B-25W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nModify the settings.py file to limit the number of pages scraped:\nCLOSESPIDER_PAGECOUNT = 20"
  },
  {
    "objectID": "posts/Web-Scraping/index.html#implementing-the-scraper",
    "href": "posts/Web-Scraping/index.html#implementing-the-scraper",
    "title": "üé¨ Web Scraping for Movie Recommendations",
    "section": "",
    "text": "We will implement a three-step scraper to:\n\nExtract actor links from a movie‚Äôs cast page.\nVisit each actor‚Äôs profile and extract their acting roles.\nFilter out non-acting roles and store only relevant movies & TV shows.\n\nüìú tmdb_spider.py (Scrapy Code)\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = \"tmdb_spider\"\n\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        \"\"\"Initialize the spider with a specific movie subdirectory from TMDB.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n    def parse(self, response):\n        \"\"\"Step 1: Navigate to the Full Cast & Crew page of the movie.\"\"\"\n        cast_url = response.url.rstrip(\"/\") + \"/cast\"\n        yield scrapy.Request(url=cast_url, callback=self.parse_full_credits)\n\n    def parse_full_credits(self, response):\n        \"\"\"Step 2: Extract all actor profile links and visit each actor's page.\"\"\"\n        actors = set(response.css(\"ol.people.credits:not(.crew) li a::attr(href)\").getall())  # Exclude crew members\n        for actor in actors:\n            #need callback=self.parse_actor_page to tell Scrapy to wait for the response before calling the function\n            yield response.follow(actor, callback=self.parse_actor_page)\n\n    def parse_actor_page(self, response):\n        \"\"\"Step 3: Extract the actor's name and their acting roles (only from the first Acting table).\"\"\"\n        actor_name = response.css(\"h2.title a::text, h1.title::text\").get()\n\n        movies = set(response.xpath(\n            \"//h3[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'acting')]\"\n            \"/following-sibling::table[1]//td[contains(@class, 'role')]/a/bdi/text()\"\n        ).getall())\n\n        for movie in movies:\n            yield {\"actor\": actor_name.strip() if actor_name else \"Unknown\", \"movie_or_TV_name\": movie.strip()}"
  },
  {
    "objectID": "posts/Web-Scraping/index.html#parsing-methods-explained",
    "href": "posts/Web-Scraping/index.html#parsing-methods-explained",
    "title": "üé¨ Web Scraping for Movie Recommendations",
    "section": "",
    "text": "The parse() method is responsible for extracting actor links from a movie‚Äôs cast page. It assumes that the response is a movie page and uses CSS selectors to locate all cast members. Once the actor links are found, it follows each link and passes the response to parse_full_credits(), ensuring that the scraper retrieves detailed information about each actor‚Äôs filmography.\n\n\n\n\nThe parse_full_credits() method extracts the full list of movies and TV shows that an actor has appeared in. It assumes that the response comes from an actor‚Äôs profile page and navigates through the filmography section using CSS selectors. Each extracted role is then passed to parse_actor_page(), allowing the scraper to process only relevant acting roles and filter out other credits such as directing or producing.\n\n\n\n\nThe parse_actor_page() method processes the details of a movie or TV show that an actor has been involved in. It assumes that the response corresponds to a movie or TV show page and extracts the title along with the actor‚Äôs name. Additionally, it verifies whether the role is an acting role by checking the role description on the page. If the role qualifies, it yields a dictionary containing the actor‚Äôs name and the corresponding movie or TV show title, ensuring that only acting credits are stored in the final dataset."
  },
  {
    "objectID": "posts/Web-Scraping/index.html#running-the-scraper",
    "href": "posts/Web-Scraping/index.html#running-the-scraper",
    "title": "üé¨ Web Scraping for Movie Recommendations",
    "section": "",
    "text": "To run the scraper for a specific movie like ‚ÄúHarry Potter and the Philosopher‚Äôs Stone‚Äù, use:\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosophe\nThis will generate a CSV file with two columns:\n\nactor: Name of the actor\nmovie_or_TV_name: Movies or TV shows they have appeared in"
  },
  {
    "objectID": "posts/Web-Scraping/index.html#making-recommendations",
    "href": "posts/Web-Scraping/index.html#making-recommendations",
    "title": "üé¨ Web Scraping for Movie Recommendations",
    "section": "",
    "text": "Now that we have the data, we can process it to find movies and TV shows that share actors with our favorite movie.\n\n\nWe use Pandas to count and sort the movies with the most shared actors:\n\n\nCode\nimport pandas as pd\n\n# Load scraped data\ndf = pd.read_csv(\"results.csv\")\n\n# Count occurrences of each movie/TV show\nrecommendations = df['movie_or_TV_name'].value_counts().reset_index()\nrecommendations.columns = ['Movie/TV Show', 'Number of Shared Actors']"
  },
  {
    "objectID": "posts/Web-Scraping/index.html#visualization",
    "href": "posts/Web-Scraping/index.html#visualization",
    "title": "üé¨ Web Scraping for Movie Recommendations",
    "section": "",
    "text": "To better understand the recommendations, we can create a bar chart:\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Plot top recommendations\nplt.figure(figsize=(10, 5))\nplt.barh(recommendations['Movie/TV Show'][:10], recommendations['Number of Shared Actors'][:10])\nplt.xlabel(\"Number of Shared Actors\")\nplt.ylabel(\"Movie/TV Show\")\nplt.title(\"Top 10 Movies/TV Shows with Shared Actors\")\nplt.gca().invert_yaxis()\nplt.show()"
  },
  {
    "objectID": "posts/Web-Scraping/index.html#summary",
    "href": "posts/Web-Scraping/index.html#summary",
    "title": "üé¨ Web Scraping for Movie Recommendations",
    "section": "",
    "text": "We built a web scraper using Scrapy to find shared actors between movies.\nWe extracted acting roles only from TMDB and stored them in a CSV file.\nWe used Pandas to count and Matplotlib to visualize the most frequently shared actors.\nThis approach provides a simple recommendation system based on shared cast members."
  }
]